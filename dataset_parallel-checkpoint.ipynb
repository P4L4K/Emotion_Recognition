{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9352d54-9cc6-4452-a82c-c199417d9451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed  # For parallel processing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight  # For class weighting\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization, Activation, Bidirectional, LSTM, Attention, Concatenate, GlobalAveragePooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 1: Define the Data Path and Emotion Labels\n",
    "data_path = \"C:\\\\Users\\\\HP\\\\Downloads\\\\RAVDESS_Dataset\"  # Path to the extracted folder\n",
    "\n",
    "emotion_labels = {\n",
    "    '01': 'neutral',\n",
    "    '02': 'calm',\n",
    "    '03': 'happy',\n",
    "    '04': 'sad',\n",
    "    '05': 'angry',\n",
    "    '06': 'fearful',\n",
    "    '07': 'disgust',\n",
    "    '08': 'surprised'\n",
    "}\n",
    "def augment_audio(y, sr):\n",
    "    \"\"\"Applies various audio augmentations to the given audio sample.\"\"\"\n",
    "    noise = np.random.randn(len(y))\n",
    "    y_noise = y + 0.005 * noise\n",
    "    y_stretch = librosa.effects.time_stretch(y, rate=np.random.uniform(0.8, 1.2))  # Variable time stretch\n",
    "    y_pitch = librosa.effects.pitch_shift(y=y, sr=sr, n_steps=np.random.randint(-3, 4))  # Random pitch shift\n",
    "    y_vol = y * np.random.uniform(0.8, 1.2)  # Random volume increase/decrease\n",
    "    shift = np.roll(y, np.random.randint(1000, 5000))  # Random time shift\n",
    "    return [y_noise, y_stretch, y_pitch, y_vol, shift]\n",
    "def extract_features_with_augmentation(file_path):\n",
    "    \"\"\"Extracts features from original and augmented audio files.\"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, duration=3, offset=0.5)  # Load the audio file\n",
    "        augmented_samples = augment_audio(y, sr)\n",
    "        features = []\n",
    "        \n",
    "        for sample in [y] + augmented_samples:\n",
    "            # MFCC features\n",
    "            mfcc = librosa.feature.mfcc(y=sample, sr=sr, n_mfcc=40)\n",
    "            # Chroma features\n",
    "            chroma = librosa.feature.chroma_stft(y=sample, sr=sr)\n",
    "            # Mel spectrogram features\n",
    "            mel = librosa.feature.melspectrogram(y=sample, sr=sr)\n",
    "            # Add more feature extraction methods here if needed\n",
    "            zcr = librosa.feature.zero_crossing_rate(sample)\n",
    "            spec_contrast = librosa.feature.spectral_contrast(y=sample, sr=sr)\n",
    "            tonnetz = librosa.feature.tonnetz(y=librosa.effects.harmonic(sample), sr=sr)\n",
    "            feature_vector = np.hstack([np.mean(mfcc, axis=1), np.mean(chroma, axis=1), np.mean(mel, axis=1),\n",
    "                                        np.mean(zcr, axis=1), np.mean(spec_contrast, axis=1), np.mean(tonnetz, axis=1)])\n",
    "\n",
    "            # Concatenate all features\n",
    "            feature_vector = np.hstack([np.mean(mfcc, axis=1), np.mean(chroma, axis=1), np.mean(mel, axis=1)])\n",
    "            features.append(feature_vector)\n",
    "\n",
    "        # Aggregate features for original and augmented samples\n",
    "        return np.vstack(features)\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting features from {file_path}. Error: {e}\")\n",
    "        return None\n",
    "# Step 4: Parallel Feature Extraction\n",
    "def process_file(file_path, emotion):\n",
    "    augmented_feature_vectors = extract_features_with_augmentation(file_path)\n",
    "    if augmented_feature_vectors is not None:\n",
    "        return [(feature_vector, emotion) for feature_vector in augmented_feature_vectors]\n",
    "    return []\n",
    "\n",
    "files_to_process = []\n",
    "for actor_folder in os.listdir(data_path):\n",
    "    actor_path = os.path.join(data_path, actor_folder)\n",
    "    if os.path.isdir(actor_path):\n",
    "        for file_name in os.listdir(actor_path):\n",
    "            if len(file_name.split(\"-\")) > 2 and file_name.endswith(\".wav\"):\n",
    "                emotion_code = file_name.split(\"-\")[2]\n",
    "                if emotion_code in emotion_labels:\n",
    "                    emotion = emotion_labels[emotion_code]\n",
    "                    file_path = os.path.join(actor_path, file_name)\n",
    "                    files_to_process.append((file_path, emotion))\n",
    "\n",
    "results = Parallel(n_jobs=-1)(delayed(process_file)(file_path, emotion) for file_path, emotion in files_to_process)\n",
    "# Flatten results and prepare data\n",
    "features, labels = [], []\n",
    "for result in results:\n",
    "    if result:\n",
    "        for feature_vector, emotion in result:\n",
    "            features.append(feature_vector)\n",
    "            labels.append(emotion)\n",
    "features_df = pd.DataFrame(features)\n",
    "features_df['label'] = labels\n",
    "label_encoder = LabelEncoder()\n",
    "features_df['label'] = label_encoder.fit_transform(features_df['label'])\n",
    "\n",
    "X = features_df.iloc[:, :-1].values\n",
    "y = features_df.iloc[:, -1].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train.reshape(X_train.shape[0], -1)).reshape(X_train.shape)\n",
    "X_test = scaler.transform(X_test.reshape(X_test.shape[0], -1)).reshape(X_test.shape)\n",
    "\n",
    "# Step 7: Split Data into Training and Testing Sets\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "# Step 8: Convert Labels to One-Hot Encoding\n",
    "y_train_one_hot = to_categorical(y_train)\n",
    "y_test_one_hot = to_categorical(y_test) \n",
    "# Compute class weights to handle class imbalance\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "class_weights_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "print(\"Preprocessed data saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45ace3df-edc2-4353-990f-56c60331f77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save the preprocessed data\n",
    "dump((X_train, X_test, y_train_one_hot, y_test_one_hot, class_weights_dict), 'preprocessed_data2.joblib')\n",
    "\n",
    "print(\"Preprocessed data saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53733655-6883-4050-b2aa-761cebfba056",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f0d8b3-c60e-4094-abb2-94367a6d19e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-14 17:49:34.513977: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-14 17:49:47.240894: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1731606593.115650   16690 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1731606594.430573   16690 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-14 17:50:02.558701: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from joblib import dump\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define paths and labels\n",
    "data_path = \"dataset_files//audio_speech_actors_01-24\"  # Update this path\n",
    "emotion_labels = {\n",
    "    '01': 'neutral', '02': 'calm', '03': 'happy', '04': 'sad',\n",
    "    '05': 'angry', '06': 'fearful', '07': 'disgust', '08': 'surprised'\n",
    "}\n",
    "\n",
    "# Function to augment audio\n",
    "def augment_audio(y, sr):\n",
    "    noise = np.random.randn(len(y)) * 0.005\n",
    "    y_stretch = librosa.effects.time_stretch(y, rate=np.random.uniform(0.8, 1.2))\n",
    "    y_pitch = librosa.effects.pitch_shift(y, sr=sr, n_steps=np.random.randint(-3, 4))\n",
    "    return [y + noise, y_stretch, y_pitch]\n",
    "\n",
    "# Extract sequential features\n",
    "def extract_features_sequential(file_path, max_len=180):\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, duration=3, offset=0.5)\n",
    "        samples = [y] + augment_audio(y, sr)\n",
    "        features = []\n",
    "        \n",
    "        for sample in samples:\n",
    "            mfcc = librosa.feature.mfcc(y=sample, sr=sr, n_mfcc=40)\n",
    "            chroma = librosa.feature.chroma_stft(y=sample, sr=sr)\n",
    "            mel = librosa.feature.melspectrogram(y=sample, sr=sr)\n",
    "            \n",
    "            # Ensure each feature array is of length max_len\n",
    "            mfcc = librosa.util.fix_length(mfcc, max_len, axis=1)\n",
    "            chroma = librosa.util.fix_length(chroma, max_len, axis=1)\n",
    "            mel = librosa.util.fix_length(mel, max_len, axis=1)\n",
    "            \n",
    "            # Stack the features into a single time-series\n",
    "            feature_vector = np.vstack([mfcc, chroma, mel]).T\n",
    "            features.append(feature_vector)\n",
    "        \n",
    "        return np.array(features)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Process each file in the dataset\n",
    "def process_files_sequential():\n",
    "    files = []\n",
    "    labels = []\n",
    "    for actor in os.listdir(data_path):\n",
    "        actor_folder = os.path.join(data_path, actor)\n",
    "        if os.path.isdir(actor_folder):\n",
    "            for file_name in os.listdir(actor_folder):\n",
    "                if file_name.endswith(\".wav\"):\n",
    "                    emotion_code = file_name.split(\"-\")[2]\n",
    "                    if emotion_code in emotion_labels:\n",
    "                        emotion = emotion_labels[emotion_code]\n",
    "                        file_path = os.path.join(actor_folder, file_name)\n",
    "                        features = extract_features_sequential(file_path)\n",
    "                        if features is not None:\n",
    "                            files.extend(features)  # Collect all samples, including augmented\n",
    "                            labels.extend([emotion] * features.shape[0])\n",
    "    return np.array(files), labels\n",
    "\n",
    "# Main function to prepare and save data\n",
    "def prepare_sequential_data():\n",
    "    features, labels = process_files_sequential()\n",
    "\n",
    "    # Check if any features were extracted\n",
    "    if not len(features):\n",
    "        print(\"No features extracted. Check data paths and feature extraction.\")\n",
    "        return\n",
    "\n",
    "    # Label encoding and one-hot encoding for labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    labels_encoded = label_encoder.fit_transform(labels)\n",
    "    labels_one_hot = to_categorical(labels_encoded)\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels_one_hot, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Scaling features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train.reshape(X_train.shape[0], -1)).reshape(X_train.shape)\n",
    "    X_test = scaler.transform(X_test.reshape(X_test.shape[0], -1)).reshape(X_test.shape)\n",
    "\n",
    "    # Compute class weights for handling imbalanced classes\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(labels_encoded), y=labels_encoded)\n",
    "    class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "    # Save the preprocessed data and metadata\n",
    "    dump((X_train, X_test, y_train, y_test, class_weights_dict), 'sequential_preprocessed_data.joblib')\n",
    "    dump(label_encoder, 'label_encoder.joblib')\n",
    "    dump(scaler, 'scaler.joblib')\n",
    "    print(\"Sequential data preprocessing and saving complete.\")\n",
    "\n",
    "# Run the data preparation\n",
    "prepare_sequential_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ac0d8a-ba60-400d-83f6-5dd41d30c4fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
